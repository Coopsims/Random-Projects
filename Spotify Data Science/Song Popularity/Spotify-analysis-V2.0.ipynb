{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard DS imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import ast\n",
    "from sklearn import svm\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# All pytorch dependencies\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "\n",
    "\n",
    "# all tensorflow dependencies\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import metrics\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Global var for selecting popularity cutoff\n",
    "MINIMUM_POPULARITY_LIMIT = 40\n",
    "MINIMUM_POPULARITY_PERCENTAGE = MINIMUM_POPULARITY_LIMIT / 100"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e544ebf3c0ec59bd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def analyze_algorithm(y_true, y_pred):\n",
    "\n",
    "    # Calculate all scoring metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Accuracy: {round(accuracy, 6)}\")\n",
    "    \n",
    "    \n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Precision: {round(precision, 6)}\")\n",
    "    \n",
    "    \n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Recall: {round(recall, 6)}\")\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"F1 Score: {round(f1, 6)}\")\n",
    "    \n",
    "\n",
    "    precision_auc, recall_auc, _ = precision_recall_curve(y_true, y_pred)\n",
    "    auprc = auc(recall_auc, precision_auc)\n",
    "    print(f\"AUPRC: {round(auprc, 6)}\")\n",
    "\n",
    "\n",
    "    # Visualize the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def print_correlation_matrix(dataframe):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = dataframe.corr()\n",
    "    \n",
    "    # Generate a mask for the upper triangle\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    \n",
    "    # Set up the matplotlib figure\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True,)\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict_and_convert(model, x_data, y_data, cutoff):\n",
    "\n",
    "    y_scores = model.predict(x_data)\n",
    "    y_pred = (y_scores > cutoff).astype(int)\n",
    "    y_data = (y_data > cutoff).astype(int)\n",
    "    return y_data, y_pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_best_kernel_value(is_classifier, x_train, y_train, x_test, y_test):\n",
    "    k_values = range(1,50,1)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    scores_dict = {}\n",
    "\n",
    "    for k in tqdm(k_values):\n",
    "        model = neighbors.KNeighborsClassifier(k) if is_classifier else neighbors.KNeighborsRegressor(k)\n",
    "        model.fit(x_train, y_train.values.ravel())\n",
    "\n",
    "        y_train_predicted = model.predict(x_train)\n",
    "        y_test_predicted = model.predict(x_test)\n",
    "\n",
    "        if is_classifier:\n",
    "            train_scores.append(f1_score(y_train, y_train_predicted, average='macro'))\n",
    "            test_score = f1_score(y_test, y_test_predicted, average='macro')\n",
    "        else:\n",
    "            y_train_binary = (y_train >= MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "            y_train_predicted_binary = (y_train_predicted >= MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "            y_test_binary = (y_test >= MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "            y_test_predicted_binary = (y_test_predicted >= MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "            \n",
    "            # Then, compute the accuracy\n",
    "            train_scores.append(f1_score(y_train_binary, y_train_predicted_binary, average='macro'))\n",
    "            test_score = f1_score(y_test_binary, y_test_predicted_binary, average='macro')\n",
    "\n",
    "        test_scores.append(test_score)\n",
    "        scores_dict[k] = test_score\n",
    "\n",
    "    plt.plot(k_values, train_scores, 'r-s', label='Train')\n",
    "    plt.plot(k_values, test_scores, 'b-o', label='Test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    best_k = max(scores_dict, key=scores_dict.get)\n",
    "    print(f\"The best k value is : {best_k} with score: {scores_dict[best_k]}\")\n",
    "\n",
    "    return best_k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f1_score_nn(y_true, y_pred):\n",
    "    # Calculate Precision and Recall\n",
    "    precision = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) / K.sum(K.round(K.clip(y_pred, 0, 1)) + K.epsilon())\n",
    "    recall = K.sum(K.round(K.clip(y_true * y_pred, 0, 1))) / K.sum(K.round(K.clip(y_true, 0, 1)) + K.epsilon())\n",
    "\n",
    "    # Calculate F1 Score\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    return f1_val"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c207c541e2da0a93",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Putting file specific info into arrays for easier access\n",
    "files = ['tracks.csv', 'dataset.csv']\n",
    "inputs = [\n",
    "            [ 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'explicit'],\n",
    "                  \n",
    "            ['track_genre_encoded', 'danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness', 'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'time_signature', 'explicit']\n",
    "        ]\n",
    "target = ['popularity']\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for i in range(len(files)):\n",
    "\n",
    "    \n",
    "\n",
    "    df = pd.read_csv('./'+files[i])\n",
    "    print(files[i], 'read in successfully as a dataframe')\n",
    "    # File specific cleaning\n",
    "    \n",
    "    # Data cleaning for tracks.csv\n",
    "    if i == 0:\n",
    "        df['release_date'] = pd.to_datetime(df['release_date'], errors='coerce').dt.year\n",
    "        df = df.drop(columns=['id', 'name', 'artists', 'id_artists'])\n",
    "        \n",
    "    # Data cleaning for dataset.csv\n",
    "    if i == 1:\n",
    "        df['track_genre_encoded'] = LabelEncoder().fit_transform(df['track_genre'])\n",
    "        df = df.drop(columns=['Unnamed: 0', 'track_id', 'artists', 'album_name', 'track_name', 'track_genre'])\n",
    "        \n",
    "       \n",
    "    print_correlation_matrix(df)\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(df[inputs[i]], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "    # Training Model 1 KNN-Regressor\n",
    "    kernelNumber = find_best_kernel_value(False, xtrain, ytrain, xtest, ytest)\n",
    "\n",
    "    modelOne = neighbors.KNeighborsRegressor(kernelNumber)\n",
    "    modelOne.fit(xtrain, ytrain)\n",
    "    print('KNN-Regressor Fitted')\n",
    "\n",
    "    # Apply a same threshold to train predictions\n",
    "    y_train, y_train_pred = predict_and_convert(modelOne, xtrain, ytrain, MINIMUM_POPULARITY_LIMIT)\n",
    "    print('\\nscores for the training set:')\n",
    "    analyze_algorithm(y_train, y_train_pred)\n",
    "\n",
    "    y_test, y_test_pred = predict_and_convert(modelOne, xtest, ytest, MINIMUM_POPULARITY_LIMIT)\n",
    "    print('\\nscores for the testing set:')\n",
    "    analyze_algorithm(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Training Model 2 KNN-Classifier\n",
    "    y_train_two = (ytrain > MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "    y_test_two = (ytest > MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "\n",
    "    kernelNumber = find_best_kernel_value(True, xtrain, y_train_two, xtest, y_test_two)\n",
    "\n",
    "    modelTwo = neighbors.KNeighborsClassifier(kernelNumber)\n",
    "    modelTwo.fit(xtrain, y_train_two.values.ravel())\n",
    "    print('KNN-Classifier Fitted')\n",
    "\n",
    "    y_train_pred = modelTwo.predict(xtrain)\n",
    "    print('\\nscores for the training set:')\n",
    "    analyze_algorithm(y_train_two, y_train_pred)\n",
    "\n",
    "    # Apply a same threshold to test predictions\n",
    "    y_test_pred = modelTwo.predict(xtest)\n",
    "    print('\\nscores for the testing set:')\n",
    "    analyze_algorithm(y_test_two, y_test_pred)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    # Training Model 3 SVM-rbf\n",
    "    # The first dataset is too big to reasonably run with SVM as it generally take O(n^3*d) time for training\n",
    "    if i == 1:\n",
    "        # Training Model 3 SVM\n",
    "        y_train = (ytrain > MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "        y_test = (ytest > MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(xtrain)\n",
    "        X_test = sc.transform(xtest)\n",
    "\n",
    "        # Training the SVM model on the Training set using a linear kernel\n",
    "        print('SVM-Classifier Training...')\n",
    "        modelThree = SVC(kernel = 'rbf', random_state = 42)\n",
    "        modelThree.fit(X_train, y_train.values.ravel())  \n",
    "        print('SVM-Classifier Fitted')\n",
    "\n",
    "        # Predicting the results\n",
    "        print('\\nscores for the training set:')\n",
    "        y_pred = modelThree.predict(X_train)\n",
    "        analyze_algorithm(y_train, y_pred)\n",
    "\n",
    "        print('\\nscores for the testing set:')\n",
    "        y_pred = modelThree.predict(X_test)\n",
    "        analyze_algorithm(y_test, y_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Training Model 4 Convolution Neural Network\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[inputs[i]], df[target], test_size=0.2, random_state=42)\n",
    "    y_train = (y_train > MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "    y_test = (y_test > MINIMUM_POPULARITY_LIMIT).astype(int)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    y_train = y_train.astype('float32')\n",
    "    y_test = y_test.astype('float32')\n",
    "    \n",
    "    # Define the neural network structure\n",
    "    modelFour = Sequential([\n",
    "        Dense(32, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    modelFour.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=[f1_score_nn])\n",
    "\n",
    "    y_train_numpy = y_train.to_numpy().astype('int64')\n",
    "    unique_classes, class_counts = np.unique(y_train_numpy, return_counts=True)\n",
    "    class_weights_dict = {class_label: len(y_train_numpy) / count for class_label, count in zip(unique_classes, class_counts)}\n",
    "\n",
    "\n",
    "    # pass the class weights to model.fit\n",
    "    modelFour.fit(X_train, y_train, epochs=2, validation_split=0.2, batch_size=100, class_weight=class_weights_dict)\n",
    "\n",
    "    # Evaluate the model\n",
    "    modelFour.evaluate(X_test, y_test)\n",
    "\n",
    "    # Make predictions\n",
    "\n",
    "    predictions = modelFour.predict(X_test)\n",
    "    plt.hist(predictions, bins=20)\n",
    "    plt.xlabel('Predicted probabilities')\n",
    "    plt.ylabel('Count')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    predictions = (modelFour.predict(X_train) > 0.5).astype(\"int64\")\n",
    "    print('\\nscores for the Training set:')\n",
    "    analyze_algorithm(y_train, predictions)\n",
    "\n",
    "\n",
    "    predictions = (modelFour.predict(X_test) > 0.5).astype(\"int64\")\n",
    "    print('\\nscores for the Testing set:')\n",
    "    analyze_algorithm(y_test, predictions)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Training Model 5 Random Forest Classifier\n",
    "    modelFive = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "    print('Random Forest Classifier Training...')\n",
    "    modelFive.fit(X_train, y_train.values.ravel())\n",
    "    print('Random Forest Classifier Fitted')\n",
    "    \n",
    "    y_train_pred = modelFive.predict(X_train)\n",
    "    print('\\nscores for the training set:')\n",
    "    analyze_algorithm(y_train, y_train_pred)\n",
    "    \n",
    "    y_test_pred = modelFive.predict(X_test)\n",
    "    print('\\nscores for the testing set:')\n",
    "    analyze_algorithm(y_test, y_test_pred)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# End timing and print the result\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nExecution time: {} seconds\".format(end_time - start_time))\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7399acd008764c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e24838f2c4b43a6d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
