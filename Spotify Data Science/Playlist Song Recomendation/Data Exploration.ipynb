{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:58:00.842095Z",
     "start_time": "2024-01-20T20:58:00.256023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Directory containing JSON files\n",
    "# directory = './spotify_million_playlist_dataset/data'\n",
    "directory = './Test Playlist'\n",
    "# Initialize data structures\n",
    "playlists_data = []\n",
    "unique_songs = {}  # Dictionary to store unique songs with URI as key\n",
    "playlist_song_map = {}  # Mapping of playlist name to song URIs\n",
    "\n",
    "# Read each JSON file and extract data\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            for playlist in data['playlists']:\n",
    "                # Extract playlist-level features\n",
    "                playlist_info = {\n",
    "                    'name': playlist['name'],\n",
    "                    'num_tracks': playlist['num_tracks'],\n",
    "                    'num_albums': playlist['num_albums'],\n",
    "                    'num_followers': playlist['num_followers'],\n",
    "                    'num_edits': playlist['num_edits'],\n",
    "                    'duration_ms': playlist['duration_ms'],\n",
    "                    'num_artists': playlist['num_artists']\n",
    "                }\n",
    "                playlists_data.append(playlist_info)\n",
    "\n",
    "                # Initialize song list for this playlist\n",
    "                playlist_song_map[playlist['name']] = []\n",
    "\n",
    "                # Extract and store unique track-level features\n",
    "                for track in playlist['tracks']:\n",
    "                    track_uri = track['track_uri']\n",
    "                    if track_uri not in unique_songs:\n",
    "                        unique_songs[track_uri] = {\n",
    "                            'artist_name': track['artist_name'],\n",
    "                            'track_name': track['track_name'],\n",
    "                            'duration_ms': track['duration_ms']\n",
    "                        }\n",
    "                    playlist_song_map[playlist['name']].append(track_uri)\n",
    "\n",
    "# Convert playlists data and unique songs to DataFrames\n",
    "playlists_df = pd.DataFrame(playlists_data)\n",
    "unique_songs_df = pd.DataFrame(unique_songs.values(), index=unique_songs.keys())\n",
    "\n",
    "# Now, playlists_df, unique_songs_df, and playlist_song_map are ready for further processing"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:58:01.164902Z",
     "start_time": "2024-01-20T20:58:00.843183Z"
    }
   },
   "id": "65e72c7458ca2fc2",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "               000  00s  100  123   14   15   16   17   18  1967  ...   \nparty party_0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...  \\\nsummer_1       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   \nRap_2          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   \n#tb_3          0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   \nDisney_4       0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0.0  ...   \n\n               End Credits (Aladdin And The King Of Thieves)   \nparty party_0                                            NaN  \\\nsummer_1                                                 NaN   \nRap_2                                                    NaN   \n#tb_3                                                    NaN   \nDisney_4                                                 NaN   \n\n               Desperado - (Live)  Cheapest Flight  Policeman - Radio Edit   \nparty party_0                 NaN              NaN                     NaN  \\\nsummer_1                      NaN              NaN                     NaN   \nRap_2                         NaN              NaN                     NaN   \n#tb_3                         NaN              NaN                     NaN   \nDisney_4                      NaN              NaN                     NaN   \n\n               Alive - Oh Snap It’s Luke! Remix  Hair  Find Love (feat. Dboy)   \nparty party_0                               NaN   NaN                     NaN  \\\nsummer_1                                    NaN   NaN                     NaN   \nRap_2                                       NaN   NaN                     NaN   \n#tb_3                                       NaN   NaN                     NaN   \nDisney_4                                    NaN   NaN                     NaN   \n\n               17 (feat. Packy)  Generation Away  Latkes  \nparty party_0               NaN              NaN     NaN  \nsummer_1                    NaN              NaN     NaN  \nRap_2                       NaN              NaN     NaN  \n#tb_3                       NaN              NaN     NaN  \nDisney_4                    NaN              NaN     NaN  \n\n[5 rows x 64713 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>000</th>\n      <th>00s</th>\n      <th>100</th>\n      <th>123</th>\n      <th>14</th>\n      <th>15</th>\n      <th>16</th>\n      <th>17</th>\n      <th>18</th>\n      <th>1967</th>\n      <th>...</th>\n      <th>End Credits (Aladdin And The King Of Thieves)</th>\n      <th>Desperado - (Live)</th>\n      <th>Cheapest Flight</th>\n      <th>Policeman - Radio Edit</th>\n      <th>Alive - Oh Snap It’s Luke! Remix</th>\n      <th>Hair</th>\n      <th>Find Love (feat. Dboy)</th>\n      <th>17 (feat. Packy)</th>\n      <th>Generation Away</th>\n      <th>Latkes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>party party_0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>summer_1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Rap_2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>#tb_3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Disney_4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 64713 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming `playlists_df`, `unique_songs_df`, and `playlist_song_map` are already defined as per the user's code\n",
    "\n",
    "# Step 1: Tokenize Playlist Names\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "playlist_name_tfidf = tfidf_vectorizer.fit_transform(playlists_df['name'])\n",
    "\n",
    "# Step 2: Create Artist and Song Features Efficiently\n",
    "# Create a list of all unique artists and songs\n",
    "all_artists = set(artist for song in unique_songs.values() for artist in song['artist_name'])\n",
    "all_songs = set(song['track_name'] for song in unique_songs.values())\n",
    "\n",
    "# Convert sets to lists\n",
    "all_artists_list = list(all_artists)\n",
    "all_songs_list = list(all_songs)\n",
    "\n",
    "# Ensure unique playlist names (if necessary, adjust this based on your data)\n",
    "playlists_df['unique_playlist_id'] = playlists_df['name'] + '_' + playlists_df.index.astype(str)\n",
    "\n",
    "# Create artist presence data\n",
    "artist_presence = {(playlist_id, unique_songs[track_uri]['artist_name']): 1\n",
    "                   for playlist_id, tracks in playlist_song_map.items()\n",
    "                   for track_uri in tracks}\n",
    "multi_index_artists = pd.MultiIndex.from_tuples(artist_presence.keys(), names=['playlist_id', 'artist'])\n",
    "artist_series = pd.Series(artist_presence, index=multi_index_artists)\n",
    "artist_matrix = artist_series.unstack(fill_value=0)\n",
    "artist_matrix = artist_matrix.reindex(columns=all_artists_list, fill_value=0)\n",
    "\n",
    "# Create song presence data\n",
    "song_presence = {(playlist_id, unique_songs[track_uri]['track_name']): 1\n",
    "                 for playlist_id, tracks in playlist_song_map.items()\n",
    "                 for track_uri in tracks}\n",
    "multi_index_songs = pd.MultiIndex.from_tuples(song_presence.keys(), names=['playlist_id', 'song'])\n",
    "song_series = pd.Series(song_presence, index=multi_index_songs)\n",
    "song_matrix = song_series.unstack(fill_value=0)\n",
    "song_matrix = song_matrix.reindex(columns=all_songs_list, fill_value=0)\n",
    "\n",
    "# Combine Features\n",
    "combined_features = pd.concat([\n",
    "    pd.DataFrame(playlist_name_tfidf.toarray(), index=playlists_df['unique_playlist_id'], columns=tfidf_vectorizer.get_feature_names_out()),\n",
    "    artist_matrix,\n",
    "    song_matrix\n",
    "], axis=1)\n",
    "\n",
    "# The `combined_features` DataFrame is now ready for unsupervised learning\n",
    "combined_features.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T20:59:23.888382Z",
     "start_time": "2024-01-20T20:59:22.311250Z"
    }
   },
   "id": "8d55525838c19593",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m song_playlist_matrix \u001B[38;5;241m=\u001B[39m combined_features\u001B[38;5;241m.\u001B[39mvalues\n\u001B[0;32m----> 2\u001B[0m \u001B[43msong_playlist_matrix\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mhead\u001B[49m()\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "song_playlist_matrix = combined_features.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:30:39.529788Z",
     "start_time": "2024-01-20T21:30:38.615335Z"
    }
   },
   "id": "e9c1e0bafdf72255",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nNMF does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 8\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# apply matrix factorisation using Non-negative Matrix Factorisation (NMF)\u001B[39;00m\n\u001B[1;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m NMF(n_components\u001B[38;5;241m=\u001B[39mn_factors)\n\u001B[0;32m----> 8\u001B[0m playlist_factors \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msong_playlist_matrix\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# playlist_factors holds the 'tastes' of our playlists\u001B[39;00m\n\u001B[1;32m      9\u001B[0m song_factors \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mcomponents_\u001B[38;5;241m.\u001B[39mT  \u001B[38;5;66;03m# song_factors holds the 'features' of our songs\u001B[39;00m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    143\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[1;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    146\u001B[0m         )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/decomposition/_nmf.py:1561\u001B[0m, in \u001B[0;36mNMF.fit_transform\u001B[0;34m(self, X, y, W, H)\u001B[0m\n\u001B[1;32m   1535\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Learn a NMF model for the data X and returns the transformed data.\u001B[39;00m\n\u001B[1;32m   1536\u001B[0m \n\u001B[1;32m   1537\u001B[0m \u001B[38;5;124;03mThis is more efficient than calling fit followed by transform.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;124;03m    Transformed data.\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m-> 1561\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1562\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat64\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat32\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m   1563\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1565\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(assume_finite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m   1566\u001B[0m     W, H, n_iter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_transform(X, W\u001B[38;5;241m=\u001B[39mW, H\u001B[38;5;241m=\u001B[39mH)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:565\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[1;32m    563\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mValidation should be done on X, y or both.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    564\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 565\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    566\u001B[0m     out \u001B[38;5;241m=\u001B[39m X\n\u001B[1;32m    567\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:921\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    915\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    916\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    917\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m    918\u001B[0m         )\n\u001B[1;32m    920\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m--> 921\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    922\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    923\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    924\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    929\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:161\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[1;32m    145\u001B[0m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[1;32m    147\u001B[0m     msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    148\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    149\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    159\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    160\u001B[0m     )\n\u001B[0;32m--> 161\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains NaN.\nNMF does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# number of factors\n",
    "n_factors = 20\n",
    "\n",
    "# apply matrix factorisation using Non-negative Matrix Factorisation (NMF)\n",
    "model = NMF(n_components=n_factors)\n",
    "playlist_factors = model.fit_transform(song_playlist_matrix) # playlist_factors holds the 'tastes' of our playlists\n",
    "song_factors = model.components_.T  # song_factors holds the 'features' of our songs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:31:43.979362Z",
     "start_time": "2024-01-20T21:31:42.921152Z"
    }
   },
   "id": "df5c6532ef64285e",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "50f3b1c420997c5c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "randomprojects",
   "language": "python",
   "display_name": "RandomProjects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
